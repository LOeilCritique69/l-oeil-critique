#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Script d'automatisation pour r√©cup√©rer les bandes-annonces depuis CineHorizons et TMDb,
les fusionner dans un bloc HTML standardis√©, puis pousser automatiquement sur GitHub.

Auteur : Yanis (L'≈íil Critique)
Version : 2.1 ‚Äî Optimis√©e pour stabilit√©, performance et maintenance.
"""

import os
import re
import json
import logging
import subprocess
from datetime import datetime
from pathlib import Path
from typing import List, Tuple, Optional, Dict
from urllib.parse import urljoin
from contextlib import contextmanager

import requests
from bs4 import BeautifulSoup
from playwright.sync_api import sync_playwright, TimeoutError as PlaywrightTimeout

# ------------------------------
# CONFIGURATION GLOBALE
# ------------------------------
SCRIPT_DIR = Path(__file__).resolve().parent
ROOT_DIR = SCRIPT_DIR.parent
LOG_FILE = SCRIPT_DIR / "bande_annonces_log.json"
OUTPUT_FILE = ROOT_DIR / "bande_annonces_blocs.html"

LIST_CINE_URL = "https://www.cinehorizons.net/bandes-annonces-prochains-films"
TMDB_API_KEY = "2cf75db44f938aeaf1e7d873a38fdcaa"
TMDB_UPCOMING_URL = "https://api.themoviedb.org/3/movie/upcoming"

MAX_BANDES_CINE = 3
MAX_BANDES_TMDB = 3
MAX_SYNOPSIS_LEN = 500
REQUEST_TIMEOUT = 10
PAGE_TIMEOUT = 15000

# ------------------------------
# LOGGING
# ------------------------------
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler(SCRIPT_DIR / "scraper.log", encoding="utf-8")
    ]
)
logger = logging.getLogger(__name__)

# ------------------------------
# OUTILS G√âN√âRIQUES
# ------------------------------
def clean_text(text: str) -> str:
    """Nettoie et normalise un texte."""
    if not text:
        return ""
    return ' '.join(text.strip().split())

def summarize_synopsis(synopsis: str, max_len: int = MAX_SYNOPSIS_LEN) -> str:
    """Raccourcit le synopsis s'il d√©passe la limite."""
    synopsis = clean_text(synopsis)
    if len(synopsis) <= max_len:
        return synopsis
    return synopsis[:max_len].rsplit(' ', 1)[0] + "..."

def load_log() -> List[str]:
    """Charge la liste des identifiants d√©j√† ajout√©s."""
    if not LOG_FILE.exists():
        return []
    
    try:
        with LOG_FILE.open("r", encoding="utf-8") as f:
            data = json.load(f)
            return data if isinstance(data, list) else []
    except (json.JSONDecodeError, IOError) as e:
        logger.warning(f"‚ö†Ô∏è Erreur lecture log : {e}. R√©initialisation.")
        return []

def save_log(log: List[str]) -> None:
    """Sauvegarde la liste mise √† jour des identifiants."""
    try:
        with LOG_FILE.open("w", encoding="utf-8") as f:
            json.dump(log, f, ensure_ascii=False, indent=2)
    except IOError as e:
        logger.error(f"‚ùå Impossible de sauvegarder le log : {e}")

def format_date(date_str: str, input_format: str = "%Y-%m-%d") -> str:
    """Formate une date au format fran√ßais."""
    try:
        date_obj = datetime.strptime(date_str, input_format)
        # Traduction manuelle des mois en fran√ßais
        mois_fr = [
            "janvier", "f√©vrier", "mars", "avril", "mai", "juin",
            "juillet", "ao√ªt", "septembre", "octobre", "novembre", "d√©cembre"
        ]
        return f"{date_obj.day} {mois_fr[date_obj.month - 1]} {date_obj.year}"
    except (ValueError, IndexError):
        return date_str

@contextmanager
def get_requests_session():
    """Context manager pour g√©rer les sessions requests."""
    session = requests.Session()
    session.headers.update({
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    })
    try:
        yield session
    finally:
        session.close()

# ------------------------------
# G√âN√âRATION HTML
# ------------------------------
def generate_article_html(
    titre: str,
    date_sortie: str,
    synopsis: str,
    iframe_html: str,
    date_ajout: Optional[str] = None
) -> str:
    """G√©n√®re le HTML d'un article de bande-annonce."""
    if date_ajout is None:
        date_ajout = datetime.now().strftime("%d %B %Y")
    
    # √âchappement HTML basique
    titre = titre.replace('<', '&lt;').replace('>', '&gt;')
    synopsis = synopsis.replace('<', '&lt;').replace('>', '&gt;')
    
    return f"""
<article class="card-bande">
    <span class="badge-nouveau">NOUVEAU</span>
    <h2>{titre}</h2>
    <p class="date-sortie">Sortie pr√©vue : {date_sortie}</p>
    <p class="ajout-site">Ajout√© le : {date_ajout}</p>
    <p class="synopsis">{synopsis}</p>
    <div class="video-responsive">{iframe_html}</div>
</article>
""".strip()

# ------------------------------
# SCRAPER CINEHORIZONS
# ------------------------------
def extract_cinehorizons_detail(page, detail_url: str, titre: str) -> Optional[Dict[str, str]]:
    """Extrait les d√©tails d'une page CineHorizons."""
    try:
        page.goto(detail_url, timeout=PAGE_TIMEOUT)
        page.wait_for_selector(".block-synopsis", timeout=8000)
    except PlaywrightTimeout:
        logger.warning(f"‚è± Timeout pour {titre}")
        return None
    except Exception as e:
        logger.error(f"‚ùå Erreur chargement page {titre}: {e}")
        return None

    detail = BeautifulSoup(page.content(), "html.parser")
    
    # Extraction date de sortie
    date_elem = detail.select_one(".movie-release span")
    date_sortie = clean_text(date_elem.text) if date_elem else "Date inconnue"
    
    # Extraction synopsis
    syn_tag = detail.select_one(".block-synopsis .field-item.even p")
    synopsis = summarize_synopsis(syn_tag.text) if syn_tag else "Pas de synopsis"
    
    # Extraction iframe
    iframe = detail.select_one(".ba .player iframe")
    if iframe and iframe.get("src"):
        iframe_html = f'<iframe width="560" height="315" src="{iframe["src"]}" frameborder="0" allowfullscreen></iframe>'
    else:
        iframe_html = "<!-- Pas d'iframe -->"
        logger.warning(f"‚ö†Ô∏è Pas d'iframe trouv√©e pour {titre}")
    
    return {
        "date_sortie": date_sortie,
        "synopsis": synopsis,
        "iframe_html": iframe_html
    }

def scrape_cinehorizons(log: List[str]) -> Tuple[List[str], List[str]]:
    """Scrape CineHorizons pour obtenir les nouvelles bandes-annonces."""
    articles, ids = [], []
    date_ajout = datetime.now().strftime("%d %B %Y")

    logger.info("üé¨ [CineHorizons] D√©marrage du scraping...")

    try:
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=True)
            context = browser.new_context(
                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            )
            page = context.new_page()
            
            page.goto(LIST_CINE_URL, timeout=PAGE_TIMEOUT)
            page.wait_for_selector(".view-content .views-row", timeout=10000)

            soup = BeautifulSoup(page.content(), "html.parser")
            blocs = soup.select(".view-content .views-row")[:MAX_BANDES_CINE]

            for i, bloc in enumerate(blocs, 1):
                link = bloc.select_one('h3[itemprop="name"] a[href]')
                if not link:
                    logger.warning(f"‚ö†Ô∏è Bloc {i} sans lien, ignor√©")
                    continue

                titre = clean_text(link.text)
                detail_url = urljoin(LIST_CINE_URL, link["href"])
                identifiant = f"cinehorizons::{titre}::{detail_url}"

                if identifiant in log:
                    logger.debug(f"‚ÑπÔ∏è {titre} d√©j√† pr√©sent, ignor√©")
                    continue

                details = extract_cinehorizons_detail(page, detail_url, titre)
                if not details:
                    continue

                article_html = generate_article_html(
                    titre=titre,
                    date_sortie=details["date_sortie"],
                    synopsis=details["synopsis"],
                    iframe_html=details["iframe_html"],
                    date_ajout=date_ajout
                )

                articles.append(article_html)
                ids.append(identifiant)
                logger.info(f"‚úÖ Ajout√© : {titre}")

            browser.close()

    except Exception as e:
        logger.error(f"‚ùå Erreur CineHorizons : {e}", exc_info=True)

    logger.info(f"‚úÖ [CineHorizons] {len(articles)} nouvelles bandes-annonces trouv√©es.")
    return articles, ids

# ------------------------------
# SCRAPER TMDB
# ------------------------------
def fetch_tmdb_trailer(session: requests.Session, movie_id: int) -> Optional[str]:
    """R√©cup√®re l'ID de la bande-annonce YouTube pour un film TMDb."""
    try:
        r = session.get(
            f"https://api.themoviedb.org/3/movie/{movie_id}/videos",
            params={"api_key": TMDB_API_KEY, "language": "fr-FR"},
            timeout=REQUEST_TIMEOUT
        )
        r.raise_for_status()
        videos = r.json().get("results", [])
        
        # Prioriser les trailers officiels
        trailer = next(
            (v for v in videos if v.get("type") == "Trailer" and v.get("site") == "YouTube"),
            None
        )
        return trailer.get("key") if trailer else None
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Erreur r√©cup√©ration trailer pour film {movie_id}: {e}")
        return None

def scrape_tmdb(log: List[str]) -> Tuple[List[str], List[str]]:
    """Scrape TMDb pour les prochains films avec trailers."""
    logger.info("üéû [TMDb] D√©marrage du scraping...")
    articles, ids = [], []
    date_ajout = datetime.now().strftime("%d %B %Y")

    try:
        with get_requests_session() as session:
            r = session.get(
                TMDB_UPCOMING_URL,
                params={"api_key": TMDB_API_KEY, "language": "fr-FR", "region": "FR"},
                timeout=REQUEST_TIMEOUT
            )
            r.raise_for_status()
            movies = r.json().get("results", [])
    except Exception as e:
        logger.error(f"‚ùå Erreur requ√™te TMDb : {e}")
        return articles, ids

    with get_requests_session() as session:
        for movie in movies[:MAX_BANDES_TMDB * 2]:  # R√©cup√©rer plus pour compenser ceux sans trailer
            if len(articles) >= MAX_BANDES_TMDB:
                break
            
            titre = movie.get("title", "Titre inconnu")
            date_sortie = format_date(movie.get("release_date", "Date inconnue"))
            movie_id = movie.get("id")
            
            if not movie_id:
                continue

            video_id = fetch_tmdb_trailer(session, movie_id)
            if not video_id:
                logger.debug(f"‚ÑπÔ∏è Pas de trailer pour {titre}")
                continue

            identifiant = f"tmdb::{titre}::{video_id}"
            if identifiant in log:
                logger.debug(f"‚ÑπÔ∏è {titre} d√©j√† pr√©sent, ignor√©")
                continue

            iframe_html = f'<iframe width="560" height="315" src="https://www.youtube.com/embed/{video_id}" frameborder="0" allowfullscreen></iframe>'
            synopsis = summarize_synopsis(movie.get("overview", "Pas de synopsis"))

            article_html = generate_article_html(
                titre=titre,
                date_sortie=date_sortie,
                synopsis=synopsis,
                iframe_html=iframe_html,
                date_ajout=date_ajout
            )

            articles.append(article_html)
            ids.append(identifiant)
            logger.info(f"‚úÖ Ajout√© : {titre}")

    logger.info(f"‚úÖ [TMDb] {len(articles)} nouvelles bandes-annonces ajout√©es.")
    return articles, ids

# ------------------------------
# PUSH GITHUB
# ------------------------------
def push_to_github() -> None:
    """Pousse automatiquement les mises √† jour sur le d√©p√¥t GitHub."""
    try:
        repo_root = SCRIPT_DIR.parent.parent
        os.chdir(repo_root)

        subprocess.run(["git", "config", "user.name", "LOeilCritique69"], check=True)
        subprocess.run(["git", "config", "user.email", "yanisfoa69@gmail.com"], check=True)
        subprocess.run(["git", "add", "."], check=True)

        status = subprocess.run(["git", "status", "--porcelain"], capture_output=True, text=True)
        if status.stdout.strip():
            subprocess.run(["git", "commit", "-m", "MAJ automatique des bandes-annonces"], check=True)
            subprocess.run(["git", "push", "-f", "origin", "main"], check=True)
            logger.info("‚úÖ Push GitHub r√©ussi.")
        else:
            logger.info("‚ÑπÔ∏è Aucun changement d√©tect√©, push annul√©.")
    except Exception as e:
        logger.error(f"‚ùå Erreur GitHub : {e}")

# ------------------------------
# MAIN
# ------------------------------
def main():
    """Point d'entr√©e principal du script."""
    logger.info("=" * 60)
    logger.info("üöÄ Lancement du script de mise √† jour des bandes-annonces")
    logger.info("=" * 60)
    
    # Chargement du log
    log = load_log()
    logger.info(f"üìã {len(log)} bandes-annonces d√©j√† en base")

    # Scraping
    cine_articles, cine_ids = scrape_cinehorizons(log)
    tmdb_articles, tmdb_ids = scrape_tmdb(log)

    nouveaux_articles = cine_articles + tmdb_articles
    nouveaux_ids = cine_ids + tmdb_ids

    if not nouveaux_articles:
        logger.info("‚ÑπÔ∏è Aucune nouvelle bande-annonce d√©tect√©e.")
        return

    # Fusion avec l'ancien contenu
    ancien_contenu = OUTPUT_FILE.read_text(encoding="utf-8") if OUTPUT_FILE.exists() else ""
    anciens_articles = re.findall(
        r'(<article class="card-bande"[^>]*>.*?</article>)',
        ancien_contenu,
        flags=re.DOTALL
    )

    # Les nouveaux articles en premier
    all_articles = nouveaux_articles + anciens_articles

    # Sauvegarde
    try:
        OUTPUT_FILE.write_text("\n\n".join(all_articles), encoding="utf-8")
        logger.info(f"üíæ Fichier HTML mis √† jour : {OUTPUT_FILE}")
    except IOError as e:
        logger.error(f"‚ùå Impossible d'√©crire le fichier HTML : {e}")
        return

    # Mise √† jour du log
    save_log(log + nouveaux_ids)

    # Push GitHub
    logger.info(f"‚úÖ {len(nouveaux_articles)} nouvelles bandes-annonces ajout√©es.")
    push_to_github()
    
    logger.info("=" * 60)
    logger.info("‚úÖ Script termin√© avec succ√®s")
    logger.info("=" * 60)

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        logger.info("\n‚ö†Ô∏è Script interrompu par l'utilisateur")
    except Exception as e:
        logger.critical(f"üí• Erreur critique : {e}", exc_info=True)
        exit(1)